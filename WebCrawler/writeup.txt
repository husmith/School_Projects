Crawler.py is a web crawler that uses a breadth-first search to return a dictionary showing the outgoing links from a webpage. The arguments given to the program are the url of the webpage your search starts from, and the depth of how many links you want to follow. If you give the argument a depth of 2, crawler.py will execute breadth-first searches of pages that are 'two clicks' away from the starting url. 
This program imports the sys, urllib.request, and urllib.parse, and html.parser libraries. THe urllib.request module urlopen() is used to open the web page of the site being passed to the function crawl(), while the urllib.parse modules urlparse, urlunparse, and urljoin are used to break the site url into parts and the join the parts together again. The module HTMLParser from the html.parser library is used to create the Collector class, which inherits the HTMLParser attributes. 
The Collector class is passed a url address and initializes the parser, the url, and a list. It's handle_starttag() method is passed the tag and attrs, which adds the hyperlink URLs to the self.links list in there absolute format. The getLinks() method returns hyperlinks URLs in their absolute format by returning the list of links created by the handle_starttag() method.
The crawl() function takes in three arguments: site, graph, and diet. 'site' refers to the url of the site that you want to do a breadth-first search from. 'graph' is the dictionary object created to show the relationships between the starting site and the 'child' sites. 
'dist' is the user-specified depth for while the search will be carried out. In order to construct an url from the site name given, urlunparse() is passed 'http' and the site name in order to create a followable url. 'content' reads the html code from the webpage that is opened from 's', and a Collector object is created for the instance of 's'. The Collector's list of links on the webpage is then iterated through and the netloc of each link is added to the graph dictionary of the site. If the depth is greater than one, then crawl() is recursively called on every link that has not already been in the graph (creating a loop). This function returns the graph of the key:value mapping between sites and the pages linked to. 
The function print_graph() takes in the graph produced by crawl(), and prints a graph with a node on each line. 